{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Convolutional Neural Network to identify fruits__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2> Import required packages</h2><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import all the necessary packages and functions\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential, save_model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Load the data</h2></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(folder_name):\n",
    "    \"\"\"\n",
    "    This function traverses the given directory, opens each sub-folder and reads all the image files one by one into a list. \n",
    "    This list is converted into a numpy array.\n",
    "   \n",
    "    Parameters:\n",
    "    folder_name - Relative path of directory that needs to be traversed\n",
    "    \n",
    "    Returns:\n",
    "    x - A numpy array of size (num_images, 100, 100, 3) where num_images is total number of images read\n",
    "    \"\"\"\n",
    "    x=[]\n",
    "    for folder in os.listdir(folder_name):\n",
    "        for file in os.listdir(os.path.join(folder_name,folder)):\n",
    "            x.append(cv2.imread(os.path.join(folder_name,folder,file)))\n",
    "    x = np.array(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(folder_name):\n",
    "    \"\"\"\n",
    "    This function traverses the given directory. Each sub-folder is opened and for each file inside it, the name of the sub-folder\n",
    "    is added to the label vector\n",
    "    \n",
    "    Parameters:\n",
    "    folder_name - Relative path of directory that needs to be traversed\n",
    "    \n",
    "    Returns:\n",
    "    y - A vector containing labels corresponding to each image in the sub-folders\n",
    "    \"\"\"\n",
    "    y=[]\n",
    "    for folder in os.listdir(folder_name):\n",
    "        for file in os.listdir(os.path.join(folder_name,folder)):\n",
    "            y.append(str(folder))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test data directories\n",
    "train_folder = 'data/fruits-360/Training'\n",
    "test_folder = 'data/fruits-360/Test'\n",
    "\n",
    "## Create numpy arrays of train and test images\n",
    "train_data = read_data(train_folder)\n",
    "test_data = read_data(test_folder)\n",
    "\n",
    "## Create label array for train and test images\n",
    "train_labels = read_labels(train_folder)\n",
    "test_labels = read_labels(test_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>One-hot encoding of labels</h2></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create one-hot vectors of labels using LabelBinarizer class of sklearn.  \n",
    "lb = LabelBinarizer()\n",
    "## Fit the LabelBinarizer to training labels\n",
    "lb.fit(train_labels)\n",
    "# Transform the label vectors\n",
    "y_train = lb.transform(train_labels)\n",
    "y_test = lb.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Shuffling training data</h2></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset using shuffle function of sklearn\n",
    "X,Y=shuffle(train_data, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Splitting into validation and test set after shuffling</h2></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, Y_1 = shuffle(test_data, y_test, random_state=0)\n",
    "X_val = X_1[:7000,:,:,:]\n",
    "Y_val = Y_1[:7000,]\n",
    "X_test = X_1[7000:,:,:,:]\n",
    "Y_test = Y_1[7000:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Defining the model</h2></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    '''\n",
    "    Create a convolutional neural network model using Keras.\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(axis=-1, input_shape=[100,100,3]))\n",
    "    model.add(Conv2D(64, (5, 5), padding='same', activation='relu', data_format='channels_last'))\n",
    "    model.add(Conv2D(64, (5, 5), padding='same', activation='relu', data_format='channels_last'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    " \n",
    "    model.add(Conv2D(128, (5, 5), padding='same', activation='relu', data_format='channels_last'))\n",
    "    model.add(Conv2D(128, (5, 5), padding='same', activation='relu', data_format='channels_last'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    \n",
    "    model.add(Conv2D(128, (5, 5), padding='same', activation='relu', data_format='channels_last'))\n",
    "    model.add(Conv2D(128, (5, 5), padding='same', activation='relu', data_format='channels_last'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    " \n",
    "    model.add(Conv2D(256, (5, 5), padding='same', activation='relu', data_format='channels_last'))\n",
    "    model.add(Conv2D(256, (5, 5), padding='same', activation='relu', data_format='channels_last'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(81, activation='softmax'))\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create model instance\n",
    "model1 = createModel()\n",
    "## Compile the model\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Model summary</h2><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 100, 100, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 64)      4864      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 128)       204928    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 50, 128)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 25, 25, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 25, 25, 128)       409728    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 25, 128)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 256)       819456    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 81)                41553     \n",
      "=================================================================\n",
      "Total params: 8,762,525\n",
      "Trainable params: 8,761,367\n",
      "Non-trainable params: 1,158\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Training the model using GPU</h2></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41322 samples, validate on 7000 samples\n",
      "Epoch 1/2\n",
      "41322/41322 [==============================] - 1553s 38ms/step - loss: 0.5292 - acc: 0.8544 - val_loss: 3.1076 - val_acc: 0.5316\n",
      "Epoch 2/2\n",
      "41322/41322 [==============================] - 1542s 37ms/step - loss: 0.1083 - acc: 0.9674 - val_loss: 0.3143 - val_acc: 0.9190\n"
     ]
    }
   ],
   "source": [
    "## Use gpu to train the model and perform validation on test data\n",
    "with tf.device('/gpu:0'):\n",
    "    model1.fit(X, Y, batch_size=32, epochs=2, verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>The model achieved 96% training set accuracy and 91% validation set accuracy in just 2 epochs. Training for few more epochs would definitely help improve the accuracy. Let's see how well it performs on the test set.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h2>Evaluating the model on test set</h2></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6877/6877 [==============================] - 83s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32147629967809016, 0.9126072414690661]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>The model achieved an accuracy of 91% on the test set. It's not anything to be excited about, but I'm positive that the model can perform better if trained for more epochs.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to a HDF5 file\n",
    "save_model(model1, 'fruit_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
